{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name_or_path = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "model_name_or_path = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Answer the user request.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Bureau/gpu-rl/env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/alex/Bureau/gpu-rl/env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/alex/Bureau/gpu-rl/env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt= [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": \"Write a Triton GPU kernel that sums two vectors. Use the Triton language, in Python.\"},\n",
    "        ]\n",
    "text = tokenizer.apply_chat_template(\n",
    "            prompt,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=512, do_sample=False)\n",
    "generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Below is an example of a Triton GPU kernel that sums two vectors using the Triton language and written in Python.\n",
      "\n",
      "```python\n",
      "import triton\n",
      "\n",
      "# Define the input tensors\n",
      "input1 = triton.Tensor(shape=(1024,), dtype=triton.float32)\n",
      "input2 = triton.Tensor(shape=(1024,), dtype=triton.float32)\n",
      "\n",
      "# Define the output tensor\n",
      "output = triton.Tensor(shape=(1024,), dtype=triton.float32)\n",
      "\n",
      "# Define the kernel function\n",
      "@triton.kernel\n",
      "def sum_vectors(input1, input2, output):\n",
      "    # Get the thread ID\n",
      "    tid = triton.thread_id()\n",
      "    \n",
      "    # Calculate the index for the current thread\n",
      "    idx = tid * 4\n",
      "    \n",
      "    # Sum the elements from input1 and input2\n",
      "    output[idx] = input1[idx] + input2[idx]\n",
      "\n",
      "# Execute the kernel\n",
      "sum_vectors(\n",
      "    input1=input1,\n",
      "    input2=input2,\n",
      "    output=output,\n",
      "    num_warps=8,\n",
      "    block_size=256,\n",
      "    grid_shape=(1,)\n",
      ")\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Input Tensors**: We define three tensors: `input1`, `input2`, and `output`. The `input1` and `input2` are of shape `(1024,)` and have a data type of `float32`.\n",
      "2. **Output Tensor**: The `output` tensor also has a shape of `(1024,)` and uses the same data type as the inputs.\n",
      "3. **Kernel Function**: The `sum_vectors` kernel is defined using the `@triton.kernel` decorator. It takes three arguments: `input1`, `input2`, and `output`.\n",
      "   - Inside the kernel, we use `triton.thread_id()` to get the thread ID.\n",
      "   - We calculate the index for the current thread by multiplying the thread ID by 4 (since each element in the vector is 4 bytes).\n",
      "   - We then sum the corresponding elements from `input1` and `input2` and store the result in the `output` tensor at the calculated index.\n",
      "4. **Execution**: Finally, we execute the kernel using the `sum_vectors` function with appropriate parameters such as `num_warps`, `block_size`, and `grid_shape`.\n",
      "\n",
      "This kernel will perform the summation of two vectors in parallel on the GPU.\n"
     ]
    }
   ],
   "source": [
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
